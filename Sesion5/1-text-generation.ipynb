{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Texto con modelos GPT\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion5/1-text-generation.ipynb)\n",
    "\n",
    "En este notebook harémos uso de un modelo tipo GPT-2 pre-entrenado en idioma español que utilizaremos para generar texto a partir de un contexto inicial que proveerémos. Luego, harémos fine tuning a este modelo con un dataset de chistes en español y observar como cambia la generación de texto en función del dataset que utilicemos.\n",
    "\n",
    "#### Referencias\n",
    "- Dataset: https://huggingface.co/datasets/mrm8488/CHISTES_spanish_jokes\n",
    "- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "- [Natural Language Processing with Transformers: Building Language Applications With Hugging Face](https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246)\n",
    "- [GPT2 Spanish](https://huggingface.co/DeepESP/gpt2-spanish)\n",
    "- [Fine-Tune a non-Englush GPT-2 Model with Huggingface](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1215907/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "installed_packages = [package.key for package in pkg_resources.working_set]\n",
    "IN_COLAB = 'google-colab' in installed_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt\n",
    "!test '{IN_COLAB}' = 'True' && sudo apt-get update -y\n",
    "!test '{IN_COLAB}' = 'True' && sudo apt-get install python3.10 python3.10-distutils python3.10-lib2to3 -y\n",
    "!test '{IN_COLAB}' = 'True' && sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.11 2\n",
    "!test '{IN_COLAB}' = 'True' && sudo update-alternatives --install /usr/local/bin/python python /usr/bin/python3.10 1\n",
    "!test '{IN_COLAB}' = 'True' && pip install lightning datasets 'transformers[torch]' sentence-transformers tochinfo evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative pre-training Transformer - GPT\n",
    "\n",
    "![](../assets/gpt.png)\n",
    "\n",
    "Los modelos tipo GPT, introducidos por Radfor, et.al., de OpenAI, al igual que los modelos BERT, hacen uso extensivo de la arquitectura de transformers como hemos estado viendo. Las diferencias claves se podrían resumir en:\n",
    "\n",
    "1. GPT utiliza bloques de **Transformer Decoder** encadenados, mientras que el modelo BERT utiliza bloques de *Transformer Encoder*\n",
    "2. GPT se centra en la generación de texto basado en un contexto, la tarea principal es la predicción del siguiente token en la secuencia, mientras que BERT se centra en el completado de partes de una secuencia, en función de un contexto anterior y posterior a la secuencia de entrada. Entonces BERT se centra en la construicción de representación de lenguage, mientras que GPT se centra en la generación de texto en función de un contexto.\n",
    "\n",
    "Sin embargo, ambos se basan en la misma premisa de pre-entrenar el modelo en tareas no-supervisadas o semi-supervisadas para que el modelo aprenda las representaciones semánticas del lenguage y luego al modelo se le pueda hacer fine tuning a tareas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"DeepESP/gpt2-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'transformer',\n",
       " 'transformer.wte',\n",
       " 'transformer.wpe',\n",
       " 'transformer.drop',\n",
       " 'transformer.h',\n",
       " 'transformer.h.0',\n",
       " 'transformer.h.0.ln_1',\n",
       " 'transformer.h.0.attn',\n",
       " 'transformer.h.0.attn.c_attn',\n",
       " 'transformer.h.0.attn.c_proj',\n",
       " 'transformer.h.0.attn.attn_dropout',\n",
       " 'transformer.h.0.attn.resid_dropout',\n",
       " 'transformer.h.0.ln_2',\n",
       " 'transformer.h.0.mlp',\n",
       " 'transformer.h.0.mlp.c_fc',\n",
       " 'transformer.h.0.mlp.c_proj',\n",
       " 'transformer.h.0.mlp.act',\n",
       " 'transformer.h.0.mlp.dropout',\n",
       " 'transformer.h.1',\n",
       " 'transformer.h.1.ln_1',\n",
       " 'transformer.h.1.attn',\n",
       " 'transformer.h.1.attn.c_attn',\n",
       " 'transformer.h.1.attn.c_proj',\n",
       " 'transformer.h.1.attn.attn_dropout',\n",
       " 'transformer.h.1.attn.resid_dropout',\n",
       " 'transformer.h.1.ln_2',\n",
       " 'transformer.h.1.mlp',\n",
       " 'transformer.h.1.mlp.c_fc',\n",
       " 'transformer.h.1.mlp.c_proj',\n",
       " 'transformer.h.1.mlp.act',\n",
       " 'transformer.h.1.mlp.dropout',\n",
       " 'transformer.h.2',\n",
       " 'transformer.h.2.ln_1',\n",
       " 'transformer.h.2.attn',\n",
       " 'transformer.h.2.attn.c_attn',\n",
       " 'transformer.h.2.attn.c_proj',\n",
       " 'transformer.h.2.attn.attn_dropout',\n",
       " 'transformer.h.2.attn.resid_dropout',\n",
       " 'transformer.h.2.ln_2',\n",
       " 'transformer.h.2.mlp',\n",
       " 'transformer.h.2.mlp.c_fc',\n",
       " 'transformer.h.2.mlp.c_proj',\n",
       " 'transformer.h.2.mlp.act',\n",
       " 'transformer.h.2.mlp.dropout',\n",
       " 'transformer.h.3',\n",
       " 'transformer.h.3.ln_1',\n",
       " 'transformer.h.3.attn',\n",
       " 'transformer.h.3.attn.c_attn',\n",
       " 'transformer.h.3.attn.c_proj',\n",
       " 'transformer.h.3.attn.attn_dropout',\n",
       " 'transformer.h.3.attn.resid_dropout',\n",
       " 'transformer.h.3.ln_2',\n",
       " 'transformer.h.3.mlp',\n",
       " 'transformer.h.3.mlp.c_fc',\n",
       " 'transformer.h.3.mlp.c_proj',\n",
       " 'transformer.h.3.mlp.act',\n",
       " 'transformer.h.3.mlp.dropout',\n",
       " 'transformer.h.4',\n",
       " 'transformer.h.4.ln_1',\n",
       " 'transformer.h.4.attn',\n",
       " 'transformer.h.4.attn.c_attn',\n",
       " 'transformer.h.4.attn.c_proj',\n",
       " 'transformer.h.4.attn.attn_dropout',\n",
       " 'transformer.h.4.attn.resid_dropout',\n",
       " 'transformer.h.4.ln_2',\n",
       " 'transformer.h.4.mlp',\n",
       " 'transformer.h.4.mlp.c_fc',\n",
       " 'transformer.h.4.mlp.c_proj',\n",
       " 'transformer.h.4.mlp.act',\n",
       " 'transformer.h.4.mlp.dropout',\n",
       " 'transformer.h.5',\n",
       " 'transformer.h.5.ln_1',\n",
       " 'transformer.h.5.attn',\n",
       " 'transformer.h.5.attn.c_attn',\n",
       " 'transformer.h.5.attn.c_proj',\n",
       " 'transformer.h.5.attn.attn_dropout',\n",
       " 'transformer.h.5.attn.resid_dropout',\n",
       " 'transformer.h.5.ln_2',\n",
       " 'transformer.h.5.mlp',\n",
       " 'transformer.h.5.mlp.c_fc',\n",
       " 'transformer.h.5.mlp.c_proj',\n",
       " 'transformer.h.5.mlp.act',\n",
       " 'transformer.h.5.mlp.dropout',\n",
       " 'transformer.h.6',\n",
       " 'transformer.h.6.ln_1',\n",
       " 'transformer.h.6.attn',\n",
       " 'transformer.h.6.attn.c_attn',\n",
       " 'transformer.h.6.attn.c_proj',\n",
       " 'transformer.h.6.attn.attn_dropout',\n",
       " 'transformer.h.6.attn.resid_dropout',\n",
       " 'transformer.h.6.ln_2',\n",
       " 'transformer.h.6.mlp',\n",
       " 'transformer.h.6.mlp.c_fc',\n",
       " 'transformer.h.6.mlp.c_proj',\n",
       " 'transformer.h.6.mlp.act',\n",
       " 'transformer.h.6.mlp.dropout',\n",
       " 'transformer.h.7',\n",
       " 'transformer.h.7.ln_1',\n",
       " 'transformer.h.7.attn',\n",
       " 'transformer.h.7.attn.c_attn',\n",
       " 'transformer.h.7.attn.c_proj',\n",
       " 'transformer.h.7.attn.attn_dropout',\n",
       " 'transformer.h.7.attn.resid_dropout',\n",
       " 'transformer.h.7.ln_2',\n",
       " 'transformer.h.7.mlp',\n",
       " 'transformer.h.7.mlp.c_fc',\n",
       " 'transformer.h.7.mlp.c_proj',\n",
       " 'transformer.h.7.mlp.act',\n",
       " 'transformer.h.7.mlp.dropout',\n",
       " 'transformer.h.8',\n",
       " 'transformer.h.8.ln_1',\n",
       " 'transformer.h.8.attn',\n",
       " 'transformer.h.8.attn.c_attn',\n",
       " 'transformer.h.8.attn.c_proj',\n",
       " 'transformer.h.8.attn.attn_dropout',\n",
       " 'transformer.h.8.attn.resid_dropout',\n",
       " 'transformer.h.8.ln_2',\n",
       " 'transformer.h.8.mlp',\n",
       " 'transformer.h.8.mlp.c_fc',\n",
       " 'transformer.h.8.mlp.c_proj',\n",
       " 'transformer.h.8.mlp.act',\n",
       " 'transformer.h.8.mlp.dropout',\n",
       " 'transformer.h.9',\n",
       " 'transformer.h.9.ln_1',\n",
       " 'transformer.h.9.attn',\n",
       " 'transformer.h.9.attn.c_attn',\n",
       " 'transformer.h.9.attn.c_proj',\n",
       " 'transformer.h.9.attn.attn_dropout',\n",
       " 'transformer.h.9.attn.resid_dropout',\n",
       " 'transformer.h.9.ln_2',\n",
       " 'transformer.h.9.mlp',\n",
       " 'transformer.h.9.mlp.c_fc',\n",
       " 'transformer.h.9.mlp.c_proj',\n",
       " 'transformer.h.9.mlp.act',\n",
       " 'transformer.h.9.mlp.dropout',\n",
       " 'transformer.h.10',\n",
       " 'transformer.h.10.ln_1',\n",
       " 'transformer.h.10.attn',\n",
       " 'transformer.h.10.attn.c_attn',\n",
       " 'transformer.h.10.attn.c_proj',\n",
       " 'transformer.h.10.attn.attn_dropout',\n",
       " 'transformer.h.10.attn.resid_dropout',\n",
       " 'transformer.h.10.ln_2',\n",
       " 'transformer.h.10.mlp',\n",
       " 'transformer.h.10.mlp.c_fc',\n",
       " 'transformer.h.10.mlp.c_proj',\n",
       " 'transformer.h.10.mlp.act',\n",
       " 'transformer.h.10.mlp.dropout',\n",
       " 'transformer.h.11',\n",
       " 'transformer.h.11.ln_1',\n",
       " 'transformer.h.11.attn',\n",
       " 'transformer.h.11.attn.c_attn',\n",
       " 'transformer.h.11.attn.c_proj',\n",
       " 'transformer.h.11.attn.attn_dropout',\n",
       " 'transformer.h.11.attn.resid_dropout',\n",
       " 'transformer.h.11.ln_2',\n",
       " 'transformer.h.11.mlp',\n",
       " 'transformer.h.11.mlp.c_fc',\n",
       " 'transformer.h.11.mlp.c_proj',\n",
       " 'transformer.h.11.mlp.act',\n",
       " 'transformer.h.11.mlp.dropout',\n",
       " 'transformer.ln_f',\n",
       " 'lm_head']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = [m for m, _ in model.named_modules()]\n",
    "modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observermos un ejemplo de generación simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la entrada: torch.Size([1, 3])\n",
      "Dimensiones de la salida: torch.Size([1, 3, 50257])\n",
      "Dimensiones del último token de la secuencia: torch.Size([50257])\n",
      "Dimensiones de la probabilidad de los tokens: torch.Size([50257])\n",
      "{' más': '40.87%', ' que': '10.55%', ' en': '7.27%', ',': '4.90%', ' allí': '0.99%', ' se': '0.87%', '.': '0.82%', ' dentro': '0.81%', ' a': '0.78%', ' al': '0.78%'}\n"
     ]
    }
   ],
   "source": [
    "text = \"Había una vez\"\n",
    "best = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n",
    "    print(\"Dimensiones de la entrada:\", tokens.shape)\n",
    "    output = model(input_ids=tokens)\n",
    "    print(\"Dimensiones de la salida:\", output.logits.shape)\n",
    "    output = output.logits[0, -1, :]\n",
    "    print(\"Dimensiones del último token de la secuencia:\", output.shape)\n",
    "    probs = torch.softmax(output, dim=-1)\n",
    "    print(\"Dimensiones de la probabilidad de los tokens:\", probs.shape)\n",
    "    sorted_probs = torch.argsort(probs, dim=-1, descending=True)\n",
    "    print({tokenizer.decode(token): f\"{prob.cpu().numpy() * 100:.2f}%\" for token, prob in zip(sorted_probs[:best], probs[sorted_probs[:best]])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando una función de generación\n",
    "\n",
    "Ahora, la idea es que este modelo nos sirva para generar texto de forma recurrente e incremental. En la última capa de los modelos tipo GPT encontrarémos un tensor con forma $(b, s, v)$, donde:\n",
    "\n",
    "- $b$: Es el tamaño del bache, o la cantidad de secuencias a procesar.\n",
    "- $s$: Es la longitud de la secuencia de entrada.\n",
    "- $v$: Es el tamaño del vocabulario del modelo, cuantos tokens soporta.\n",
    "\n",
    "Pero este es el tensor de salida, por qué tiene la forma de la secuencia de entrada?, porque cada posición en la salida corresponde a la la predicción del siguiente token de esa posición en la secuencia de entrada. En otras palabras, lo que obtenemos como predicción, es una secuencia de igual tamaño a la de entrada, movida un token hacia adelante, lo que efectivamente nos predice un solo token a la vez y por ende, el token que nos insteresa, es el último.\n",
    "\n",
    "Lo que obtenemos en este tensor es además los logits de TODO el vocabulario del modelo, con los cuales podemos calcular las probabilidades de que cada uno sea el que continue en la secuencia. Hay varias formas de decodificar el siguiente token, la más fácil de implementar sería una decodificación codiciosa (greedy) del siguiente token, que consiste simplemente en seleccionar el token con la probabilidad más alta. Este es un enfoque simple y efectivo para algunos casos, pero al mismo tiempo sufre de poca variabilidad e incluso puede caer en generación repetitiva.\n",
    "\n",
    "Otra opción es el muestreo, ya que justamente podemos obtener probabilidades del siguiente token, lo más lógico sería muestrear con esas opciones de probabilidad, de este modo podemos obtener mayor diversidad a la hora de generar el texto, al costo eso si de que haya mayor aleatoridad ya que se le daría la oportunidad a incluso tokens con baja probabilidad, de ser seleccionados.\n",
    "\n",
    "Otra opción podría ser un balanceo entre una decodificación greedy y una por muestreo, en función de otro hiperparámetro que podemos definir. Esta sería una técnica muy común en el contexto de Reinforcement Learning llamade e-greedy. Se hace la aclaración de que en este ejemplo no harémos nada de RL, solamente se hace mención de esta técnica para balancear entre explotación y exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "def generate(\n",
    "        model: nn.Module, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        start: str, \n",
    "        max_length: int = 100, \n",
    "        eps: float = 0.5, \n",
    "        top_n: int = 5,\n",
    "        return_iterations: bool = False,\n",
    "        device: str = \"cpu\") -> Tuple[str, Optional[pd.DataFrame]]:\n",
    "\n",
    "    output = [start]\n",
    "    iterations = []\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer(output[-1], return_tensors='pt')['input_ids'].to(device)\n",
    "        for _ in range(max_length):\n",
    "            # Tomamos los logits producidos por la última capa del modelo\n",
    "            # Estos corresponden al siguiente token por cada posición de la cadena\n",
    "            logits = model(input_ids=input_ids).logits\n",
    "            # Por lo tanto, el que nos interesa es el último, que correspondería a la\n",
    "            # predicción del siguiente token después del final de la cadena original\n",
    "            # A este aplicamos un softmax para obtener las probabilidades por cada\n",
    "            # token del vocabulario para estar presente en la cadena.\n",
    "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "            # Simplemente ordenamos por probabilidad de forma descendente\n",
    "            sorted_tokens = torch.argsort(probs, dim=-1, descending=True)\n",
    "\n",
    "            # Utilizamos una politica tipo e-greedy para obtener el siguiente token de la secuencia\n",
    "            # Un eps>=1 quiere decir que siempre se va seleccionar el token de forma 'greedy', es decir\n",
    "            # siempre se toma el token con probabilidad más alta.\n",
    "\n",
    "            # Un eps=0 quiere decir que siempre se va a muestrear el siguiente token en función\n",
    "            # de las probabilidades de cada token\n",
    "\n",
    "            # Un 0<eps<1 va a balancear de forma binomial entre tomar el token con la\n",
    "            # probabilidad más alta y muestrear el token en función de sus probabilidades. \n",
    "            if np.random.random_sample(1)[0] < eps:\n",
    "                # Se toma el mejor token\n",
    "                next_token = sorted_tokens[0].unsqueeze(dim=0)\n",
    "            else:\n",
    "                # Se muetrea el token de la probabilidad de distribución\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            if return_iterations:\n",
    "                # Mantenemos pista de todas las iteraciones para análisis\n",
    "                iteration = {'input': ''.join(output)}\n",
    "                best_n = sorted_tokens[:top_n].cpu().tolist()\n",
    "                choices = {f'Choice #{choice+1}': f'{tokenizer.decode(token)} ({prob:.4f})' for choice, (token, prob) in enumerate(zip(best_n, probs[best_n].cpu().tolist()))}\n",
    "                iteration.update(choices)\n",
    "                iterations.append(iteration)\n",
    "\n",
    "            output.append(tokenizer.decode(next_token))\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(dim=0)], dim=-1)\n",
    "\n",
    "        output_text = ''.join(output)\n",
    "        if not return_iterations:\n",
    "            return output_text, None\n",
    "        else:\n",
    "            df = pd.DataFrame(iterations)\n",
    "            return output_text, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos que pasa cuando generamos texto con nuestra función y algunos parámetros.\n",
    "\n",
    "Primero, observemos que pasa cuando pasamos un `eps=1` que quiere decir que la generación va a ser de tipo greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez más, el hombre que había sido su padre, el que había sido su\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>Choice #1</th>\n",
       "      <th>Choice #2</th>\n",
       "      <th>Choice #3</th>\n",
       "      <th>Choice #4</th>\n",
       "      <th>Choice #5</th>\n",
       "      <th>Choice #6</th>\n",
       "      <th>Choice #7</th>\n",
       "      <th>Choice #8</th>\n",
       "      <th>Choice #9</th>\n",
       "      <th>Choice #10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Había una vez</td>\n",
       "      <td>más (0.4087)</td>\n",
       "      <td>que (0.1055)</td>\n",
       "      <td>en (0.0727)</td>\n",
       "      <td>, (0.0490)</td>\n",
       "      <td>allí (0.0099)</td>\n",
       "      <td>se (0.0087)</td>\n",
       "      <td>. (0.0082)</td>\n",
       "      <td>dentro (0.0081)</td>\n",
       "      <td>a (0.0078)</td>\n",
       "      <td>al (0.0078)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Había una vez más</td>\n",
       "      <td>, (0.5322)</td>\n",
       "      <td>en (0.0701)</td>\n",
       "      <td>. (0.0519)</td>\n",
       "      <td>la (0.0194)</td>\n",
       "      <td>que (0.0191)</td>\n",
       "      <td>: (0.0174)</td>\n",
       "      <td>el (0.0162)</td>\n",
       "      <td>se (0.0157)</td>\n",
       "      <td>a (0.0142)</td>\n",
       "      <td>y (0.0113)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Había una vez más,</td>\n",
       "      <td>el (0.0491)</td>\n",
       "      <td>se (0.0456)</td>\n",
       "      <td>la (0.0409)</td>\n",
       "      <td>en (0.0339)</td>\n",
       "      <td>y (0.0301)</td>\n",
       "      <td>no (0.0299)</td>\n",
       "      <td>había (0.0205)</td>\n",
       "      <td>me (0.0194)</td>\n",
       "      <td>su (0.0158)</td>\n",
       "      <td>los (0.0157)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Había una vez más, el</td>\n",
       "      <td>hombre (0.0230)</td>\n",
       "      <td>hecho (0.0225)</td>\n",
       "      <td>señor (0.0173)</td>\n",
       "      <td>joven (0.0165)</td>\n",
       "      <td>recuerdo (0.0157)</td>\n",
       "      <td>muchacho (0.0120)</td>\n",
       "      <td>mundo (0.0119)</td>\n",
       "      <td>corazón (0.0105)</td>\n",
       "      <td>cuerpo (0.0095)</td>\n",
       "      <td>deseo (0.0087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Había una vez más, el hombre</td>\n",
       "      <td>que (0.1364)</td>\n",
       "      <td>se (0.0979)</td>\n",
       "      <td>de (0.0590)</td>\n",
       "      <td>había (0.0475)</td>\n",
       "      <td>no (0.0358)</td>\n",
       "      <td>le (0.0276)</td>\n",
       "      <td>estaba (0.0216)</td>\n",
       "      <td>la (0.0205)</td>\n",
       "      <td>del (0.0204)</td>\n",
       "      <td>al (0.0195)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Había una vez más, el hombre que</td>\n",
       "      <td>había (0.1757)</td>\n",
       "      <td>se (0.0684)</td>\n",
       "      <td>estaba (0.0556)</td>\n",
       "      <td>le (0.0475)</td>\n",
       "      <td>la (0.0471)</td>\n",
       "      <td>tenía (0.0348)</td>\n",
       "      <td>lo (0.0310)</td>\n",
       "      <td>me (0.0221)</td>\n",
       "      <td>no (0.0146)</td>\n",
       "      <td>amaba (0.0139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Había una vez más, el hombre que había</td>\n",
       "      <td>sido (0.0862)</td>\n",
       "      <td>estado (0.0856)</td>\n",
       "      <td>visto (0.0478)</td>\n",
       "      <td>hablado (0.0323)</td>\n",
       "      <td>conocido (0.0255)</td>\n",
       "      <td>intentado (0.0232)</td>\n",
       "      <td>hecho (0.0230)</td>\n",
       "      <td>matado (0.0214)</td>\n",
       "      <td>en (0.0184)</td>\n",
       "      <td>entrado (0.0129)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Había una vez más, el hombre que había sido</td>\n",
       "      <td>su (0.1692)</td>\n",
       "      <td>, (0.0989)</td>\n",
       "      <td>el (0.0616)</td>\n",
       "      <td>. (0.0441)</td>\n",
       "      <td>antes (0.0383)</td>\n",
       "      <td>en (0.0360)</td>\n",
       "      <td>y (0.0337)</td>\n",
       "      <td>capaz (0.0231)</td>\n",
       "      <td>durante (0.0221)</td>\n",
       "      <td>siempre (0.0183)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Había una vez más, el hombre que había sido su</td>\n",
       "      <td>padre (0.1336)</td>\n",
       "      <td>compañero (0.1296)</td>\n",
       "      <td>amigo (0.1290)</td>\n",
       "      <td>hermano (0.0368)</td>\n",
       "      <td>marido (0.0357)</td>\n",
       "      <td>primer (0.0353)</td>\n",
       "      <td>hijo (0.0245)</td>\n",
       "      <td>jefe (0.0181)</td>\n",
       "      <td>mejor (0.0178)</td>\n",
       "      <td>amante (0.0173)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>, (0.1805)</td>\n",
       "      <td>había (0.1334)</td>\n",
       "      <td>. (0.1022)</td>\n",
       "      <td>y (0.0761)</td>\n",
       "      <td>se (0.0562)</td>\n",
       "      <td>no (0.0505)</td>\n",
       "      <td>en (0.0381)</td>\n",
       "      <td>le (0.0301)</td>\n",
       "      <td>durante (0.0280)</td>\n",
       "      <td>era (0.0277)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>el (0.1232)</td>\n",
       "      <td>y (0.0614)</td>\n",
       "      <td>que (0.0339)</td>\n",
       "      <td>un (0.0325)</td>\n",
       "      <td>su (0.0319)</td>\n",
       "      <td>había (0.0298)</td>\n",
       "      <td>se (0.0236)</td>\n",
       "      <td>no (0.0193)</td>\n",
       "      <td>en (0.0161)</td>\n",
       "      <td>era (0.0159)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>que (0.2648)</td>\n",
       "      <td>hombre (0.1358)</td>\n",
       "      <td>padre (0.0359)</td>\n",
       "      <td>señor (0.0272)</td>\n",
       "      <td>hijo (0.0235)</td>\n",
       "      <td>único (0.0214)</td>\n",
       "      <td>de (0.0165)</td>\n",
       "      <td>mismo (0.0148)</td>\n",
       "      <td>más (0.0115)</td>\n",
       "      <td>hermano (0.0096)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>había (0.4517)</td>\n",
       "      <td>le (0.0950)</td>\n",
       "      <td>se (0.0635)</td>\n",
       "      <td>la (0.0389)</td>\n",
       "      <td>lo (0.0228)</td>\n",
       "      <td>no (0.0184)</td>\n",
       "      <td>estaba (0.0114)</td>\n",
       "      <td>tenía (0.0102)</td>\n",
       "      <td>siempre (0.0099)</td>\n",
       "      <td>, (0.0094)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>sido (0.1529)</td>\n",
       "      <td>estado (0.0628)</td>\n",
       "      <td>tenido (0.0455)</td>\n",
       "      <td>hecho (0.0314)</td>\n",
       "      <td>intentado (0.0235)</td>\n",
       "      <td>llevado (0.0172)</td>\n",
       "      <td>sabido (0.0169)</td>\n",
       "      <td>vivido (0.0136)</td>\n",
       "      <td>muerto (0.0135)</td>\n",
       "      <td>dado (0.0118)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Había una vez más, el hombre que había sido su...</td>\n",
       "      <td>su (0.5971)</td>\n",
       "      <td>el (0.1628)</td>\n",
       "      <td>un (0.0241)</td>\n",
       "      <td>, (0.0165)</td>\n",
       "      <td>la (0.0141)</td>\n",
       "      <td>mi (0.0110)</td>\n",
       "      <td>siempre (0.0104)</td>\n",
       "      <td>tan (0.0093)</td>\n",
       "      <td>y (0.0068)</td>\n",
       "      <td>capaz (0.0058)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input         Choice #1  \\\n",
       "0                                       Había una vez      más (0.4087)   \n",
       "1                                   Había una vez más        , (0.5322)   \n",
       "2                                  Había una vez más,       el (0.0491)   \n",
       "3                               Había una vez más, el   hombre (0.0230)   \n",
       "4                        Había una vez más, el hombre      que (0.1364)   \n",
       "5                    Había una vez más, el hombre que    había (0.1757)   \n",
       "6              Había una vez más, el hombre que había     sido (0.0862)   \n",
       "7         Había una vez más, el hombre que había sido       su (0.1692)   \n",
       "8      Había una vez más, el hombre que había sido su    padre (0.1336)   \n",
       "9   Había una vez más, el hombre que había sido su...        , (0.1805)   \n",
       "10  Había una vez más, el hombre que había sido su...       el (0.1232)   \n",
       "11  Había una vez más, el hombre que había sido su...      que (0.2648)   \n",
       "12  Había una vez más, el hombre que había sido su...    había (0.4517)   \n",
       "13  Había una vez más, el hombre que había sido su...     sido (0.1529)   \n",
       "14  Había una vez más, el hombre que había sido su...       su (0.5971)   \n",
       "\n",
       "              Choice #2         Choice #3          Choice #4  \\\n",
       "0          que (0.1055)       en (0.0727)         , (0.0490)   \n",
       "1           en (0.0701)        . (0.0519)        la (0.0194)   \n",
       "2           se (0.0456)       la (0.0409)        en (0.0339)   \n",
       "3        hecho (0.0225)    señor (0.0173)     joven (0.0165)   \n",
       "4           se (0.0979)       de (0.0590)     había (0.0475)   \n",
       "5           se (0.0684)   estaba (0.0556)        le (0.0475)   \n",
       "6       estado (0.0856)    visto (0.0478)   hablado (0.0323)   \n",
       "7            , (0.0989)       el (0.0616)         . (0.0441)   \n",
       "8    compañero (0.1296)    amigo (0.1290)   hermano (0.0368)   \n",
       "9        había (0.1334)        . (0.1022)         y (0.0761)   \n",
       "10           y (0.0614)      que (0.0339)        un (0.0325)   \n",
       "11      hombre (0.1358)    padre (0.0359)     señor (0.0272)   \n",
       "12          le (0.0950)       se (0.0635)        la (0.0389)   \n",
       "13      estado (0.0628)   tenido (0.0455)     hecho (0.0314)   \n",
       "14          el (0.1628)       un (0.0241)         , (0.0165)   \n",
       "\n",
       "              Choice #5            Choice #6          Choice #7  \\\n",
       "0         allí (0.0099)          se (0.0087)         . (0.0082)   \n",
       "1          que (0.0191)           : (0.0174)        el (0.0162)   \n",
       "2            y (0.0301)          no (0.0299)     había (0.0205)   \n",
       "3     recuerdo (0.0157)    muchacho (0.0120)     mundo (0.0119)   \n",
       "4           no (0.0358)          le (0.0276)    estaba (0.0216)   \n",
       "5           la (0.0471)       tenía (0.0348)        lo (0.0310)   \n",
       "6     conocido (0.0255)   intentado (0.0232)     hecho (0.0230)   \n",
       "7        antes (0.0383)          en (0.0360)         y (0.0337)   \n",
       "8       marido (0.0357)      primer (0.0353)      hijo (0.0245)   \n",
       "9           se (0.0562)          no (0.0505)        en (0.0381)   \n",
       "10          su (0.0319)       había (0.0298)        se (0.0236)   \n",
       "11        hijo (0.0235)       único (0.0214)        de (0.0165)   \n",
       "12          lo (0.0228)          no (0.0184)    estaba (0.0114)   \n",
       "13   intentado (0.0235)     llevado (0.0172)    sabido (0.0169)   \n",
       "14          la (0.0141)          mi (0.0110)   siempre (0.0104)   \n",
       "\n",
       "            Choice #8          Choice #9         Choice #10  \n",
       "0     dentro (0.0081)         a (0.0078)        al (0.0078)  \n",
       "1         se (0.0157)         a (0.0142)         y (0.0113)  \n",
       "2         me (0.0194)        su (0.0158)       los (0.0157)  \n",
       "3    corazón (0.0105)    cuerpo (0.0095)     deseo (0.0087)  \n",
       "4         la (0.0205)       del (0.0204)        al (0.0195)  \n",
       "5         me (0.0221)        no (0.0146)     amaba (0.0139)  \n",
       "6     matado (0.0214)        en (0.0184)   entrado (0.0129)  \n",
       "7      capaz (0.0231)   durante (0.0221)   siempre (0.0183)  \n",
       "8       jefe (0.0181)     mejor (0.0178)    amante (0.0173)  \n",
       "9         le (0.0301)   durante (0.0280)       era (0.0277)  \n",
       "10        no (0.0193)        en (0.0161)       era (0.0159)  \n",
       "11     mismo (0.0148)       más (0.0115)   hermano (0.0096)  \n",
       "12     tenía (0.0102)   siempre (0.0099)         , (0.0094)  \n",
       "13    vivido (0.0136)    muerto (0.0135)      dado (0.0118)  \n",
       "14       tan (0.0093)         y (0.0068)     capaz (0.0058)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text, iterations_df = generate(model, tokenizer, text, max_length=15, eps=1.0, top_n=10, return_iterations=True, device=device)\n",
    "print(output_text)\n",
    "iterations_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos como el input progresa a la vez que las opciones de tokens que hay. Sin importar cuantas veces invoquemos a la función con los mismos parámetros, siempre vamos a obtener los mismos resultados.\n",
    "\n",
    "Ahora, observemos que pasa si introducimos exploración al reducir el `eps=0.5`, lo cual nos dice que aproximadamente la mitad de las veces va a elegir el siguiente token muestreando y la otra mitad explotando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez más, el hombre le mostró una sonrisa que prometía ser de hasta qué punto era alegre y una pizca de humor negro. \n",
      "\n",
      "Dolores cortó su conversación con Vicente. \n",
      "\n",
      "—No para nosotros, y de lo que no tenemos nada que decirnos es que no nos hemos pasado la vida en el campo. \n",
      "\n",
      "—No hay que pensar en nada. ¿Qué te parece si nos sentamos a cenar? \n",
      "\n",
      "—No, no, no tenemos nada que hacer —dijo divertida—. De hecho\n"
     ]
    }
   ],
   "source": [
    "output_text, _ = generate(model, tokenizer, text, max_length=100, eps=0.5, device=device)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, cada vez que invoquemos a la función, vamos a obtener una respuesta diferente, a veces más coherente y otras veces menos. Vale la pena realizar varias pruebas para observar los resultados hasta encontrar un balance.\n",
    "\n",
    "### Generando texto con las utilidades del modelo\n",
    "\n",
    "Ahora, la clase de Huggingface implementa la función `generate` que hace la labor de generación por nosotros, incluyendo las opciones de muestreo y explotación como hemos observado. Solo que además permite otra serie de parámetros y opciones para controlar la generación de texto. Para más información se recomienda estudiar:\n",
    "\n",
    "- [Natural Language Processing with Transformers: Building Language Applications With Hugging Face](https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246), Capitulo 5\n",
    "- https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "- https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/text_generation#transformers.GenerationMixin.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez más, en aquel momento, un pensamiento se apoderó de él. \n",
      "\n",
      "—No es posible que yo hubiera sido tan afortunado —dijo. \n",
      "\n",
      "Pero no pudo contenerse. \n",
      "\n",
      "—Tú no eres el que se lo ha dicho a tu madre. \n",
      "\n",
      "—¿Por qué? \n",
      "\n",
      "—Porque ella se lo ha dicho a tu padre. \n",
      "\n",
      "—Pero ¿por qué? \n",
      "\n",
      "—Porque ella no lo ha dicho a tu madre. \n",
      "\n",
      "—¿Por qué?\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokens, pad_token_id=tokenizer.eos_token_id, max_length=100, do_sample=True, temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning\n",
    "\n",
    "Ahora, intentemos hacer fine tuning a nuestro modelo. Intentemos entrenarlo en un corpus de chistes en idioma español y ver como la narrativa de su output cambia.\n",
    "\n",
    "##### Nota\n",
    "Lastimosamente, este dataset es muy pequeño y la distribución del texto es muy diferente a la distribución de texto con la cual fue entrenado el modelo original, por lo que no se esperan resultados significativamente ejemplares. Sin embargo, el objetivo es observar como cambia la generación del texto una vez lo entrenamos en un conjunto especializado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'keywords', 'funny', 'category'],\n",
       "        num_rows: 2419\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mrm8488/CHISTES_spanish_jokes\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': '- ¡Rápido, necesitamos sangre!\\n- Yo soy 0 positivo.\\n- Pues muy mal, necesitamos una mentalidad optimista.',\n",
       " 'keywords': 'sangre',\n",
       " 'funny': 1,\n",
       " 'category': 'otros'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>keywords</th>\n",
       "      <th>funny</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...</td>\n",
       "      <td>sangre</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>- ¿Cuál es el mejor portero del mundial? \\n- E...</td>\n",
       "      <td>futbol,porteros</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>El otro día unas chicas llamarón a mi puerta y...</td>\n",
       "      <td>dinero,agua</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>- Andresito, ¿qué planeta va después de Marte?...</td>\n",
       "      <td>planetas</td>\n",
       "      <td>1</td>\n",
       "      <td>profesiones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>- ¿Por qué Bob Esponja no va al gimnasio? \\n- ...</td>\n",
       "      <td>esponja,gimnasios</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Van dos ciegos y le dice uno al otro: \\n- Ojal...</td>\n",
       "      <td>ciegos</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Noticia de última hora!! \\n\\nMuere una suegra ...</td>\n",
       "      <td>canarias,coches,noticias</td>\n",
       "      <td>2</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>– Mamá, mamá, en el colegio dicen que estoy lo...</td>\n",
       "      <td>locos,sillas</td>\n",
       "      <td>1</td>\n",
       "      <td>familia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>– Mamá, mamá, ¿me haces un bocata de jamón?\\n–...</td>\n",
       "      <td>madres,jamón</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>- Qué pasa si te expulsan de cuatro univerdade...</td>\n",
       "      <td>universitarios,universidades</td>\n",
       "      <td>1</td>\n",
       "      <td>otros</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0  - ¡Rápido, necesitamos sangre!\\n- Yo soy 0 pos...   \n",
       "1   1  - ¿Cuál es el mejor portero del mundial? \\n- E...   \n",
       "2   2  El otro día unas chicas llamarón a mi puerta y...   \n",
       "3   3  - Andresito, ¿qué planeta va después de Marte?...   \n",
       "4   4  - ¿Por qué Bob Esponja no va al gimnasio? \\n- ...   \n",
       "5   5  Van dos ciegos y le dice uno al otro: \\n- Ojal...   \n",
       "6   6  Noticia de última hora!! \\n\\nMuere una suegra ...   \n",
       "7   7  – Mamá, mamá, en el colegio dicen que estoy lo...   \n",
       "8   8  – Mamá, mamá, ¿me haces un bocata de jamón?\\n–...   \n",
       "9   9  - Qué pasa si te expulsan de cuatro univerdade...   \n",
       "\n",
       "                       keywords  funny     category  \n",
       "0                        sangre      1        otros  \n",
       "1               futbol,porteros      1        otros  \n",
       "2                   dinero,agua      1        otros  \n",
       "3                      planetas      1  profesiones  \n",
       "4             esponja,gimnasios      1        otros  \n",
       "5                        ciegos      1        otros  \n",
       "6      canarias,coches,noticias      2      familia  \n",
       "7                  locos,sillas      1      familia  \n",
       "8                  madres,jamón      1        otros  \n",
       "9  universitarios,universidades      1        otros  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.set_format('pandas')\n",
    "df = dataset['train'].to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Palabras por chiste'] = df['text'].str.split().apply(len)\n",
    "df['Palabras por chiste'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí podemos observar que la mediana de longitud en terminos de palabras es de 31. Esto es esperado, pues los chistes deben ser cortos por naturaleza. Por otra parte, es bastante claro que el corpus original del modelo pre-entrenado contenía texto muy diferente a este, por lo que la calidad de los resultados, sin hacer mayores modificaciones puede que no sea buena.\n",
    "\n",
    "Sin embargo, a manera demostrativa, continuarémos con el ejercicio, prepararémos el conjunto de datos para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(max_len):\n",
    "    def _preprocess_function(examples):\n",
    "        return tokenizer(examples['text'], max_length=max_len, truncation=True, padding='max_length')\n",
    "    return _preprocess_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos GPT no esperan otra cosa más que los `input_ids`, por lo que retirarémos todas las demás columnas del dataset ya que no nos son de utilidad en este momento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2177\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 242\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.reset_format()\n",
    "tokenized_dataset = dataset['train'].map(preprocess_function(max_len=512), batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([col for col in tokenized_dataset.column_names if col != 'input_ids'])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(train_size=0.9)\n",
    "tokenized_dataset.set_format('torch')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente procedemos a definir el entrenamiento. Observaremos que es muy similar a como entrenamos a BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "batch_size = 8 if IN_COLAB else 2\n",
    "logging_steps = len(tokenized_dataset['train']) // batch_size\n",
    "# Definimos los parámetros globales de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./hf-gpt',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "# Y definimos el entrenador, especificando el modelo, datasets y el tokenizador\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10890' max='10890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10890/10890 46:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.465200</td>\n",
       "      <td>3.249673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.918000</td>\n",
       "      <td>3.221956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.573900</td>\n",
       "      <td>3.238452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.303600</td>\n",
       "      <td>3.270826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.080300</td>\n",
       "      <td>3.301478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.904700</td>\n",
       "      <td>3.337730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.758100</td>\n",
       "      <td>3.367514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.651700</td>\n",
       "      <td>3.396965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.581100</td>\n",
       "      <td>3.409228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.527300</td>\n",
       "      <td>3.416162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 52s, sys: 22.5 s, total: 46min 14s\n",
      "Wall time: 46min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10890, training_loss=2.1758858590524497, metrics={'train_runtime': 2786.4402, 'train_samples_per_second': 7.813, 'train_steps_per_second': 3.908, 'total_flos': 5688327536640000.0, 'train_loss': 2.1758858590524497, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora observemos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez un hombre conduciendo un coche por una carretera secundaria.\n",
      "Al cabo de un rato el hombre le pregunta:\n",
      "- ¿Con qué coche vive?\n",
      "- Pues con el que yo quiero ir a ver.\n",
      "Y el hombre le responde:\n",
      "- ¡Ah, así que es eso!.\n",
      "El hombre le pregunta:\n",
      "- ¡Pues que si! ¡Pues que si!\n",
      "Y el hombre le responde:\n",
      "- ¡Pues que si! ¡Pues que si! ¡\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokens, pad_token_id=tokenizer.eos_token_id, max_length=100, do_sample=True, temperature=0.8)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez un joven y un niño que se llamaba: \n",
      "- Oye yo, ¿dónde estás ahora?\n",
      "- Vivo con mi madre. \n",
      "- Me alegro, pero mi madre se ha ido a vivir con mi hijo. \n",
      "- Genial, muy amable, pero no me ha dicho nada ni me ha dicho nada grave. \n",
      "- Vale, pero…yo lo divina, ¿dónde has estado a la hora de cenar? \n",
      "- Rafe Eeah. \n",
      "-\n"
     ]
    }
   ],
   "source": [
    "output_text, _ = generate(model, tokenizer, text, max_length=100, eps=0.2, device=device)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No parece ser muy gracioso precisamente, sin embargo, notemos que la generación de texto cambia de \"estilo\", ahora es mucho más frecuente encontrar conversaciones cortas, frases concisas, y situaciones particulares, en lugar del estilo más literario del modelo original. Esto es un indicio de la influencia que tiene el conjunto de datos de entrenamiento en el modelo final, esto es algo a tener muy en cuenta a la hora de utilizar y hacer fine tuning a modelos de lenguaje.\n",
    "\n",
    "## Conclusiones\n",
    "- Los modelos BERT y GPT son muy similares, aunque ambos tienen diferencias claras en cuanto a su estructura y manera de entrenamiento.\n",
    "- Sin embargo, ambos pueden utilizarse para el mismo tipo de tareas posteriores, lo cual sustenta la importancia del pre-entrenamiento y la construcción de embeddings de buena calidad.\n",
    "- En los modelos generativos, tiende a ver un dilema de tipo exploración-explitación, al explotar los resultados, podemos ser más precisos, per al mismo tiempo más monótonos, mientras que explorando podemos ser más creativos y diversos, pero al mismo tiempo terminar con texto incoherente, difuso o alucinante. Es necesario evaluar la tarea a la mano para escoger el ajuste adecuado entre estas dos técnicas de decodificación.\n",
    "- Los modelos generativos de texto no son más que una gran probabilidad de distribución y esta a su vez es completamente dependiente de los datos con los que fue entrenada. Es aquí donde se hace sumamente importante obtener y curar los conjuntos de datos con los que se entrena, de lo contrario se puede terminar con un modelo de mala calidad para la tarea en especifico.\n",
    "- Diferentes estrategias de decodificación entregan resultados diferentes, vale la pena hacer una exploración de los resultados y ajustar los hiperparámetros para obtener los resultados deseados según el objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icesi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
